---

title: Tutorial - Transformers


keywords: fastai
sidebar: home_sidebar

summary: "An example of how to incorporate the transfomers library from HuggingFace with fastai"
description: "An example of how to incorporate the transfomers library from HuggingFace with fastai"
nb_path: "nbs/39_tutorial.transformers.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/39_tutorial.transformers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this tutorial, we will see how we can use the fastai library to fine-tune a pretrained transformer model from the <a href="https://github.com/huggingface/transformers">transformers library</a> by HuggingFace. We will use the mid-level API to gather the data. Even if this tutorial is self contained, it might help to check the <a href="http://docs.fast.ai/tutorial.imagenette">imagenette tutorial</a> to have a second look on the mid-level API (with a gentle introduction using the higher level APIs) in computer vision.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Importing-a-transformers-pretrained-model">Importing a transformers pretrained model<a class="anchor-link" href="#Importing-a-transformers-pretrained-model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First things first, we will need to install the transformers library. If you haven't done it yet, install the library:</p>

<pre><code>!pip install -Uq transformers</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then let's import what will need: we will fine-tune the GPT2 pretrained model and fine-tune on wikitext-2 here. For this, we need the <code>GPT2LMHeadModel</code> (since we want a language model) and the <code>GPT2Tokenizer</code> to prepare the data.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2TokenizerFast</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre><span class="ansi-blue-intense-fg ansi-bold">wandb</span>: <span class="ansi-yellow-fg">WARNING</span> W&amp;B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use several verions of this GPT2 model, look at the <a href="https://huggingface.co/transformers/pretrained_models.html">transformers documentation</a> for more details. Here we will use the basic version (that laready takes a lot of space in memory!) You can change the model used by changing the content of <code>pretrained_weights</code> (if it's not a GPT2 model, you'll need to change the classes used for the model and the tokenizer of course).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pretrained_weights</span> <span class="o">=</span> <span class="s1">&#39;gpt2&#39;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: [&#39;h.0.attn.masked_bias&#39;, &#39;h.1.attn.masked_bias&#39;, &#39;h.2.attn.masked_bias&#39;, &#39;h.3.attn.masked_bias&#39;, &#39;h.4.attn.masked_bias&#39;, &#39;h.5.attn.masked_bias&#39;, &#39;h.6.attn.masked_bias&#39;, &#39;h.7.attn.masked_bias&#39;, &#39;h.8.attn.masked_bias&#39;, &#39;h.9.attn.masked_bias&#39;, &#39;h.10.attn.masked_bias&#39;, &#39;h.11.attn.masked_bias&#39;, &#39;lm_head.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before we move on to the fine-tuning part, let's have a look at this <code>tokenizer</code> and this <code>model</code>. The tokenizers in HuggingFace usually do the tokenization and the numericalization in one step (we ignore the padding warning for now):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;This is an example of text, and&#39;</span><span class="p">)</span>
<span class="n">ids</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[1212, 318, 281, 1672, 286, 2420, 11, 290]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like fastai <a href="https://fastcore.fast.ai/transform#Transform"><code>Transform</code></a>s, the tokenizer has a <code>decode</code> method to give you back a text from ids:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;This is an example of text, and&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The model can be used to generate predictions (it is pretrained). It has a <code>generate</code> method that expects a batch of prompt, so we feed it our ids and add one batch dimension (there is a padding warning we can ignore as well):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">ids</span><span class="p">)[</span><span class="kc">None</span><span class="p">]</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-3-71ded8accb3e&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-fg">import</span> torch
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg"> </span>t <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>LongTensor<span class="ansi-blue-fg">(</span>ids<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">[</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> preds <span class="ansi-blue-fg">=</span> model<span class="ansi-blue-fg">.</span>generate<span class="ansi-blue-fg">(</span>t<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;ids&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The predictions, by default, are of length 20:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([1, 20]),
 tensor([1212,  318,  281, 1672,  286, 2420,   11,  290,  340,  338,  407,  257,
          922,  530,   13,  198,  198,  464,  717, 1517]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use the decode method (that prefers a numpy array to a tensor):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#34;This is an example of text, and it&#39;s not a good one.\n\nThe first thing&#34;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bridging-the-gap-with-fastai">Bridging the gap with fastai<a class="anchor-link" href="#Bridging-the-gap-with-fastai"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's see how we can use fastai to fine-tune this model on wikitext-2, using all the training utilies (learning rate finder, 1cycle policy etc...). First, we import all the text utilities:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.text.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Preparing-the-data">Preparing the data<a class="anchor-link" href="#Preparing-the-data"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we download the dataset (if not present), it comes as two csv files:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">WIKITEXT_TINY</span><span class="p">)</span>
<span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [Path(&#39;/home/jhoward/.fastai/data/wikitext-2/test.csv&#39;),Path(&#39;/home/jhoward/.fastai/data/wikitext-2/train.csv&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's have a look at what those csv files look like:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">df_valid</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;test.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">df_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>\n = 2013 – 14 York City F.C. season = \n \n The 2013 – 14 season was the &lt;unk&gt; season of competitive association football and 77th season in the Football League played by York City Football Club , a professional football club based in York , North Yorkshire , England . Their 17th @-@ place finish in 2012 – 13 meant it was their second consecutive season in League Two . The season ran from 1 July 2013 to 30 June 2014 . \n Nigel Worthington , starting his first full season as York manager , made eight permanent summer signings . By the turn of the year York were only above the relegation z...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>\n = Big Boy ( song ) = \n \n " Big Boy " &lt;unk&gt; " I 'm A Big Boy Now " was the first single ever recorded by the Jackson 5 , which was released by Steeltown Records in January 1968 . The group played instruments on many of their Steeltown compositions , including " Big Boy " . The song was neither a critical nor commercial success , but the Jackson family were delighted with the outcome nonetheless . \n The Jackson 5 would release a second single with Steeltown Records before moving to Motown Records . The group 's recordings at Steeltown Records were thought to be lost , but they were re...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>\n = The Remix ( Lady Gaga album ) = \n \n The Remix is a remix album by American recording artist Lady Gaga . Released in Japan on March 3 , 2010 , it contains remixes of the songs from her first studio album , The Fame ( 2008 ) , and her third extended play , The Fame Monster ( 2009 ) . A revised version of the track list was prepared for release in additional markets , beginning with Mexico on May 3 , 2010 . A number of recording artists have produced the songs , including Pet Shop Boys , Passion Pit and The Sound of Arrows . The remixed versions feature both uptempo and &lt;unk&gt; composit...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>\n = New Year 's Eve ( Up All Night ) = \n \n " New Year 's Eve " is the twelfth episode of the first season of the American comedy television series Up All Night . The episode originally aired on NBC in the United States on January 12 , 2012 . It was written by Erica &lt;unk&gt; and was directed by Beth McCarthy @-@ Miller . The episode also featured a guest appearance from Jason Lee as Chris and Reagan 's neighbor and Ava 's boyfriend , Kevin . \n During Reagan ( Christina Applegate ) and Chris 's ( Will &lt;unk&gt; ) first New Year 's Eve game night , Reagan 's competitiveness comes out causing Ch...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>\n = Geopyxis carbonaria = \n \n Geopyxis carbonaria is a species of fungus in the genus Geopyxis , family &lt;unk&gt; . First described to science in 1805 , and given its current name in 1889 , the species is commonly known as the charcoal loving elf @-@ cup , dwarf &lt;unk&gt; cup , &lt;unk&gt; &lt;unk&gt; cup , or pixie cup . The small , &lt;unk&gt; @-@ shaped fruitbodies of the fungus are reddish @-@ brown with a whitish fringe and measure up to 2 cm ( 0 @.@ 8 in ) across . They have a short , tapered stalk . Fruitbodies are commonly found on soil where brush has recently been burned , sometimes in great numbers ....</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We gather all texts in one numpy array (since it will be easier to use this way with fastai):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">all_texts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">df_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">df_valid</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To process this data to train a model, we need to build a <a href="https://fastcore.fast.ai/transform#Transform"><code>Transform</code></a> that will be applied lazily. In this case we might could do the pre-processing once and for all and only use the transform for decoding (we will see how just after), but the fast tokenizer from HuggingFace is, as its name indicates, fast, so it doesn't really impact performance to do it this way.</p>
<p>In a fastai <a href="https://fastcore.fast.ai/transform#Transform"><code>Transform</code></a> you can define:</p>
<ul>
<li>an <code>encodes</code> method that is applied when you call the transform (a bit like the <code>forward</code> method in a <code>nn.Module</code>)</li>
<li>a <code>decodes</code> method that is applied when you call the <code>decode</code> method of the transform, if you need to decode anything for showing purposes (like converting ids to a text here)</li>
<li>a <code>setups</code> method that sets some inner state of the <a href="https://fastcore.fast.ai/transform#Transform"><code>Transform</code></a> (not needed here so we skip it)</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">TransformersTokenizer</span><span class="p">(</span><span class="n">Transform</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
    <span class="k">def</span> <span class="nf">encodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="n">toks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">toks</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">decodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">TitledStr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Two comments on the code above:</p>
<ul>
<li>in <code>encodes</code> we don't use the <code>tokenizer.encode</code> method since it does some additional preprocessing for the model after tokenizing and numericalizing (the aprt throwing a warning before). Here we don't need any post-processing so it's fine to skip it.</li>
<li>in <code>decodes</code> we return a <a href="/torch_core.html#TitledStr"><code>TitledStr</code></a> object and not jsut a plain string. That's a fastai class that adds a <code>show</code> method to the string, which will allow us to use all the fastai show methods.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can then group your data with this <a href="https://fastcore.fast.ai/transform#Transform"><code>Transform</code></a> using a <a href="/data.core.html#TfmdLists"><code>TfmdLists</code></a>. It has an s in its name because it contains the training and validation set. We indicate the indices of the training set and the validation set with <code>splits</code> (here all the first indices until <code>len(df_train)</code> and then all the remaining indices):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">range_of</span><span class="p">(</span><span class="n">df_train</span><span class="p">)),</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_texts</span><span class="p">)))]</span>
<span class="n">tls</span> <span class="o">=</span> <span class="n">TfmdLists</span><span class="p">(</span><span class="n">all_texts</span><span class="p">,</span> <span class="n">TransformersTokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> <span class="n">dl_type</span><span class="o">=</span><span class="n">LMDataLoader</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We specify <code>dl_type=LMDataLoader</code> for when we will convert this <a href="/data.core.html#TfmdLists"><code>TfmdLists</code></a> to <a href="/data.core.html#DataLoaders"><code>DataLoaders</code></a>: we will use an <a href="/text.data.html#LMDataLoader"><code>LMDataLoader</code></a> since we have a language modeling problem, not the usual fastai <a href="/data.core.html#TfmdDL"><code>TfmdDL</code></a>.</p>
<p>In a <a href="/data.core.html#TfmdLists"><code>TfmdLists</code></a> you can access to the elements of the training or validation set quite easily:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tls</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">tls</span><span class="o">.</span><span class="n">valid</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([220, 198, 796,  ..., 198, 220, 198]),
 tensor([220, 198, 796,  ..., 198, 220, 198]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>They both look the same but that just begins they begin and end the same way, we can see the shape are differents:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tls</span><span class="o">.</span><span class="n">tfms</span><span class="p">(</span><span class="n">tls</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tls</span><span class="o">.</span><span class="n">tfms</span><span class="p">(</span><span class="n">tls</span><span class="o">.</span><span class="n">valid</span><span class="o">.</span><span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4576]), torch.Size([1485]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can have a look at both decodes using <a href="/data.core.html#show_at"><code>show_at</code></a>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The fastai library expects the data to be assembled in a <a href="/data.core.html#DataLoaders"><code>DataLoaders</code></a> object (something that has a training and validation dataloader). We can get one by using the <code>dataloaders</code> method. We just have to specify a batch size and a sequence length. Since the GPT2 model was trained with sequences of size 1024, we use this sequence length (it's a stateless model, so it will change the perplexity if we use less):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span><span class="p">,</span><span class="n">sl</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span><span class="mi">1024</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">tls</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">sl</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that you may have to reduce the batch size depending on your GPU RAM.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In fastai, as soo as we have a <a href="/data.core.html#DataLoaders"><code>DataLoaders</code></a>, we can use <code>show_batch</code> to have a look at the data (here texts for inputs, and the same text shifted by one token to the right for validation):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>text_</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>\n = Ten Commandments in Catholic theology = \n \n The Ten Commandments are a series of religious and moral &lt;unk&gt; that are recognized as a moral foundation in several of the Abrahamic religions, including Catholicism. As described in the Old Testament books Exodus and &lt;unk&gt;, the Commandments form part of a covenant offered by God to the Israelites to free them from the spiritual slavery of sin. According to the Catechism of the Catholic Church — the official &lt;unk&gt; of the Catholic Church's Christian beliefs — the Commandments are considered essential for spiritual good health and growth, and serve as the basis for Catholic social justice. A review of the Commandments is one of the most common types of examination of conscience used by Catholics before receiving the sacrament of &lt;unk&gt;. \n The Commandments appear in the earliest Church writings ; the Catechism states that they have "</td>
      <td>\n = Ten Commandments in Catholic theology = \n \n The Ten Commandments are a series of religious and moral &lt;unk&gt; that are recognized as a moral foundation in several of the Abrahamic religions, including Catholicism. As described in the Old Testament books Exodus and &lt;unk&gt;, the Commandments form part of a covenant offered by God to the Israelites to free them from the spiritual slavery of sin. According to the Catechism of the Catholic Church — the official &lt;unk&gt; of the Catholic Church's Christian beliefs — the Commandments are considered essential for spiritual good health and growth, and serve as the basis for Catholic social justice. A review of the Commandments is one of the most common types of examination of conscience used by Catholics before receiving the sacrament of &lt;unk&gt;. \n The Commandments appear in the earliest Church writings ; the Catechism states that they have " occupied</td>
    </tr>
    <tr>
      <th>1</th>
      <td>@ 8 million ( US $ 7 @.@ 0 million ) in 35 days. The film completed a 50 @-@ day run in 302 centres on 18 September 2009. By then, the film had collected ₹ 650 million ( US $ 9 @.@ 7 million ) and stood strong. \n The film completed its 100 @-@ day run in 223 centres and grossed over ₹ 1 @.@ 25 billion ( US $ 19 million ) without satellite and audio rights. By then it had surpassed &lt;unk&gt;'s Sivaji ( 2007 ), which grossed ₹ 650 million ( US $ 9 @.@ 7 million ) in Tamil Nadu, and stood second to &lt;unk&gt; ( 2008 ), which reached ₹ 2 billion ( US $ 30 million ). The film completed a 175 @-@ day run in 3 centres and, by then, collected a share of ₹ 580 million ( US $ 8</td>
      <td>8 million ( US $ 7 @.@ 0 million ) in 35 days. The film completed a 50 @-@ day run in 302 centres on 18 September 2009. By then, the film had collected ₹ 650 million ( US $ 9 @.@ 7 million ) and stood strong. \n The film completed its 100 @-@ day run in 223 centres and grossed over ₹ 1 @.@ 25 billion ( US $ 19 million ) without satellite and audio rights. By then it had surpassed &lt;unk&gt;'s Sivaji ( 2007 ), which grossed ₹ 650 million ( US $ 9 @.@ 7 million ) in Tamil Nadu, and stood second to &lt;unk&gt; ( 2008 ), which reached ₹ 2 billion ( US $ 30 million ). The film completed a 175 @-@ day run in 3 centres and, by then, collected a share of ₹ 580 million ( US $ 8</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another way to gather the data is to preprocess the texts once and for all and only use the transform to decode the tensors to texts:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">toks</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">toks</span><span class="p">))</span>

<span class="n">tokenized</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenize</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">(</span><span class="n">all_texts</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='662' class='' max='662' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [662/662 00:12<00:00]
    </div>
    
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we change the previous <a href="/text.core.html#Tokenizer"><code>Tokenizer</code></a> like this:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">TransformersTokenizer</span><span class="p">(</span><span class="n">Transform</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
    <span class="k">def</span> <span class="nf">encodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="k">return</span> <span class="n">x</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">decodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">TitledStr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the <code>encodes</code> method, we still account for the case where we get something that's not already tokenized, just in case we were to build a dataset with new texts using this transform.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tls</span> <span class="o">=</span> <span class="n">TfmdLists</span><span class="p">(</span><span class="n">tokenized</span><span class="p">,</span> <span class="n">TransformersTokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> <span class="n">dl_type</span><span class="o">=</span><span class="n">LMDataLoader</span><span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">tls</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">sl</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can check it still works properly for showing purposes:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>text_</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>\n = New Year's Eve ( Up All Night ) = \n \n " New Year's Eve " is the twelfth episode of the first season of the American comedy television series Up All Night. The episode originally aired on NBC in the United States on January 12, 2012. It was written by Erica &lt;unk&gt; and was directed by Beth McCarthy @-@ Miller. The episode also featured a guest appearance from Jason Lee as Chris and Reagan's neighbor and Ava's boyfriend, Kevin. \n During Reagan ( Christina Applegate ) and Chris's ( Will &lt;unk&gt; ) first New Year's Eve game night, Reagan's competitiveness comes out causing Chris to become embarrassed. Meanwhile, Missy ( Jennifer Hall ) brings an unexpected date along to the party and, Kevin ( Jason Lee ) starts to feel as though Ava ( Maya Rudolph ) may be &lt;unk&gt; of him. \n " New Year's</td>
      <td>\n = New Year's Eve ( Up All Night ) = \n \n " New Year's Eve " is the twelfth episode of the first season of the American comedy television series Up All Night. The episode originally aired on NBC in the United States on January 12, 2012. It was written by Erica &lt;unk&gt; and was directed by Beth McCarthy @-@ Miller. The episode also featured a guest appearance from Jason Lee as Chris and Reagan's neighbor and Ava's boyfriend, Kevin. \n During Reagan ( Christina Applegate ) and Chris's ( Will &lt;unk&gt; ) first New Year's Eve game night, Reagan's competitiveness comes out causing Chris to become embarrassed. Meanwhile, Missy ( Jennifer Hall ) brings an unexpected date along to the party and, Kevin ( Jason Lee ) starts to feel as though Ava ( Maya Rudolph ) may be &lt;unk&gt; of him. \n " New Year's Eve</td>
    </tr>
    <tr>
      <th>1</th>
      <td>its peak intensity on August 28 as a Category 2 hurricane with maximum sustained winds of 100 mph ( 160 km / h ). At the same time, a reconnaissance aircraft reported a minimum barometric pressure of 991 mbar ( hPa ; 29 @.@ 27 inHg ) in the storm's eye as Edith made its closest pass to Bermuda. The hurricane began to gradually weaken after it passed east of the island, before becoming extratropical on August 31. The cyclone would later make a clockwise loop before dissipating completely late on September 3. Although Edith remained at sea, it was suspected that the hurricane may have caused the loss of the pleasure yacht &lt;unk&gt; IV, after it separated from its &lt;unk&gt;. \n \n = = = Tropical Storm Five = = = \n \n A weak disturbance was first observed near Grand Cayman on August 23, gaining tropical storm</td>
      <td>peak intensity on August 28 as a Category 2 hurricane with maximum sustained winds of 100 mph ( 160 km / h ). At the same time, a reconnaissance aircraft reported a minimum barometric pressure of 991 mbar ( hPa ; 29 @.@ 27 inHg ) in the storm's eye as Edith made its closest pass to Bermuda. The hurricane began to gradually weaken after it passed east of the island, before becoming extratropical on August 31. The cyclone would later make a clockwise loop before dissipating completely late on September 3. Although Edith remained at sea, it was suspected that the hurricane may have caused the loss of the pleasure yacht &lt;unk&gt; IV, after it separated from its &lt;unk&gt;. \n \n = = = Tropical Storm Five = = = \n \n A weak disturbance was first observed near Grand Cayman on August 23, gaining tropical storm strength</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Fine-tuning-the-model">Fine-tuning the model<a class="anchor-link" href="#Fine-tuning-the-model"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The HuggingFace model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them is some regularization scheme). To work inside the fastai training loop, we will need to drop those using a <a href="/callback.core.html#Callback"><code>Callback</code></a>: we use those to alter the behavior of the training loop.</p>
<p>Here we need to write the event <code>after_pred</code> and replace <code>self.learn.pred</code> (which contains the predictions that will be passed to the loss function) by just its first element. In callbacks, there is a shortcut that lets you access any of the underlying <a href="/learner.html#Learner"><code>Learner</code></a> attribute so we can write <code>self.pred[0]</code> instead of <code>self.learn.pred[0]</code>. That shorcut only works for read access, not write, so we have to write <code>self.learn.pred</code> on the right side (otherwise we would set a <code>pred</code> attribute in the <a href="/callback.core.html#Callback"><code>Callback</code></a>).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DropOutput</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">after_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Of course we could make this a bit more complex and add some penalty to the loss using the other part of the tuple of predictions, like the <a href="/callback.rnn.html#RNNRegularizer"><code>RNNRegularizer</code></a>.</p>
<p>Now, we are ready to create our <a href="/learner.html#Learner"><code>Learner</code></a>, which is a fastai object grouping data, model and loss function and handles model training or inference. Since we are in a language model setting, we pass perplexity as a metric, and we need to use the callback we just defined. Lastly, we use mixed precision to save every bit of memory we can (and if you have a modern GPU, it will also make training faster):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">DropOutput</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="n">Perplexity</span><span class="p">())</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can check how good the model is without any fine-tuning step (spoiler alert, it's pretty good!)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [3.2425637245178223,25.599266052246094]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This lists the validation loss and metrics (so 26.6 as perplexity is kind of amazing).</p>
<p>Now that we have a <a href="/learner.html#Learner"><code>Learner</code></a> we can use all the fastai training loop capabilities: learning rate finder, training with 1cycle etc...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.00831763744354248, lr_steep=0.0691830962896347)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The learning rate finder curve suggests picking something between 1e-4 and 1e-3.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>perplexity</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>3.646031</td>
      <td>3.244000</td>
      <td>25.636072</td>
      <td>02:44</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now with just one epoch of fine-tuning and not much regularization, our model did not really improve since it was already amazing. To have a look at some generated texts, let's take a prompt that looks like a wikipedia article:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_valid</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>\n = Tropical Storm &lt;unk&gt; ( 2008 ) = \n \n Tropical Storm &lt;unk&gt; was the tenth tropical storm of the 2008 Atlantic hurricane season . &lt;unk&gt; developed out of a strong tropical wave which moved off the African coast on August 31 . The wave quickly became organized and was declared Tropical Depression Ten while located 170 mi ( 270 km ) to the south @-@ southeast of the Cape Verde Islands on September 2 . The depression was quickly upgraded to Tropical Storm &lt;unk&gt; around noon the same day . Over the next several days , &lt;unk&gt; moved in a general west @-@ northwest direction and reached its peak...</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Article seems to begin with new line and the title between = signs, so we will mimic that:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> = Unicorn = </span><span class="se">\n</span><span class="s2"> </span><span class="se">\n</span><span class="s2"> A unicorn is a magical creature with a rainbow tail and a horn&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The prompt needs to be tokenized and numericalized, so we use the same function as before to do this, before we use the <code>generate</code> method of the model.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">prompt_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="n">inp</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">prompt_ids</span><span class="p">)[</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">inp</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 21])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;\n = Unicorn = \n \n A unicorn is a magical creature with a rainbow tail and a horn on its head.\n\nA unicorn can fly at speeds of up to 100 miles per hour&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

